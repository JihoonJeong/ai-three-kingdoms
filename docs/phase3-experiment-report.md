# Phase 3 실험 리포트: Soft Shell의 효과와 한계

> **ai-ludens Four-Shell Model 실증 검증 보고서**
> 프로젝트: AI 삼국지 — 적벽대전
> 날짜: 2026-02-17 (last updated)
> 실험 모델: Gemini 3 Flash Preview ($0.04/game)
> 총 실험: ~100게임, ~$4.20

---

## 1. Executive Summary

In-Context Learning(ICL)을 통한 Soft Shell 주입이 동일 모델의 전략 성능을 **D등급(0% 승률)에서 C등급(100% 적벽 승률)**로 끌어올렸다. 프롬프트 기반 전략 가이드(Hard Shell 강화)는 **역효과**를 발생시켰으나, 마일스톤 기반 적시 코칭(MilestoneCoach)은 **F등급을 완전히 소멸**시키고 남군 진군 병력을 0명에서 2,200명으로 끌어올렸다. 최종적으로 **커리큘럼 학습(Easy 난이도 조정)**을 통해 **B등급 최초 달성, A등급 2회 달성**이라는 돌파구를 열었다.

### 핵심 수치

| 실험 | 조건 | 등급 분포 | 적벽 승률 | B+등급 |
|------|------|----------|----------|--------|
| Phase 2 기준선 | ICL 없음, normal | 5D | 0% | 0 |
| Seed ICL 5게임 | 시드 2개, normal | 4C+1F | **100%** | 0 |
| 대조군 5게임 | ICL 없음, normal | 3D+2F | 0% | 0 |
| 순차학습 20게임 | 시드 2개→축적, normal | 13C+7F | **65%** | 0 |
| 엔진 수정 후 대조군 | 시드 3개, normal | 4C+1F | **100%** | 0 |
| Easy 일괄설교 | 시드 3개, easy | 5D | 0% | 0 |
| 엔진 수정 순차학습 20게임 | 시드 3개→축적, normal | 13C+4D+3F | **65%** | 0 |
| MilestoneCoach 5게임 | 시드 3개+코치, normal | 5C | **100%** | 0 |
| **🏆 Easy 커리큘럼 5게임** | **시드+코치+난이도조정, easy** | **2A+1B+1D+1F** | **60%** | **3** |
| Stage 2: Hard 전환 5게임 | Easy A/B 시드→hard(구normal) | 3C+2F | **100%** | 0 |
| **Stage 2.5: Medium 5게임** | **시드+코치+징병코칭, medium** | **1A+3C+1D** | **60%** | **1** |
| Stage 3: Normal 5게임 | medium A 시드→normal | 4C+1D | **80%** | 0 |

---

## 2. 실험 연대기

### 2.1 Phase 2 → Phase 3 전이: "Core ≠ Performance" 발견

Phase 2에서 7개 모델을 테스트한 결과, 모델 가격과 전략 성능 사이에 상관관계가 없었다.

```
가격순: Flash($0.04) < Haiku($0.07) ≈ o4-mini($0.07) < GPT-5($0.11) < Sonnet($0.21)
성능순: Flash = Haiku = Sonnet = GPT-5 (전패 0%) <<< o4-mini (33%)
```

**유일한 차별점**: o4-mini의 chain-of-thought reasoning 구조.
**핵심 질문**: Soft Shell(경험 축적)이 reasoning의 부재를 보완할 수 있는가?

### 2.2 실험 1: Seed ICL vs 대조군 (D→C 돌파)

**날짜**: 2026-02-16
**설계**: o4-mini 승리 패턴 2개를 시드로 주입 vs ICL 없음

| 실험 | chibiVictory | 등급 | 결과 |
|------|-------------|------|------|
| Seed ICL | 5/5 (100%) | 4C+1F | **적벽 승리 학습 완료** |
| 대조군 | 0/5 (0%) | 3D+2F | 변화 없음 |

**발견**: Seed ICL만으로 Gemini Flash가 **전혀 하지 못하던 march**를 학습. Level 1("준비만 함") → Level 2("전략적 실행")로 도약. D→C 돌파 성공.

**Four-Shell 관점**: 다른 AI(o4-mini)의 경험을 이식하는 것이 가능하며, Core 능력의 한계를 Soft Shell이 보완할 수 있다는 첫 번째 증거.

### 2.3 실험 2: 20게임 순차학습 (C→B 시도)

**날짜**: 2026-02-16
**설계**: 시드 2개 + 게임 경험 순차 축적, 20게임

```
등급 분포: 13C + 7F (B등급 0회)
적벽 승률: 65% (초반과 후반 차이 없음)
학습 곡선: 포화 — 동일 패턴 반복
```

**C→B 천장의 근본 원인 분석**:

20게임 F등급 7개를 전수 분석한 결과, **전략 차이 없이 동일한 병리 패턴** 발견:

```
적벽 승리 (턴 9~11)
  ↓
"0병력 행동" 반복 (턴 12~15):
  - transfer(강하→하구) → "병력 0명을 보급했습니다" × 5~8회
  - 강하는 적벽 진군으로 이미 비어있음 — 그런데 계속 강하에서 보급 시도
  - 하구 군량 고갈 → 매턴 300~440명 탈영
  ↓
절망적 진군 (턴 15~19):
  - march(강하→남군) → 0~수명 vs 15,250명
  - 100% 패배 → 유비 포로(20% RNG) → F등급
```

**핵심 발견 3가지**:

1. **C와 F의 차이는 전략이 아니라 RNG**: 유비 포로 확률 20%가 유일한 변수
2. **보급 방향 역전 불가**: 적벽 전 강하→하구는 올바르지만, 적벽 후 하구→강하로 전환해야 하는데 이를 학습하지 못함
3. **Closed Learning Loop**: C등급 경험만 축적 → C등급 교훈만 생성 → C등급 전략만 학습 → C등급 반복

### 2.4 실험 3: 엔진 안전장치 + B등급 합성 시드

**날짜**: 2026-02-17
**수정 사항**:
- 엔진: 0병력 transfer/march 거부 (행동 낭비 방지)
- ICL: B등급 합성 시드 추가 (적벽→보급역전→남군점령 시퀀스)

| 실험 | 조건 | 등급 | 결과 |
|------|------|------|------|
| 대조군 (시드 3개, normal) | 엔진 수정 + B시드 | 4C+1F | 기존과 동일 수준 |

엔진 수정은 정상 동작하나, B등급 합성 시드만으로는 B등급 돌파 실패.

### 2.5 실험 4: Easy 모드 (Hard Shell 강화) — **역효과 발견**

**날짜**: 2026-02-17
**설계**: EASY_STRATEGY_GUIDE(4단계 전략 가이드) + 시드 3개

```
결과: 5D (0% 적벽 승률)
→ seed만 있을 때 100%이던 적벽 승률이 0%로 추락
```

**이것이 이 리포트의 가장 중요한 발견 중 하나다.**

### 2.6 실험 5: 엔진 수정 후 20게임 순차학습

**날짜**: 2026-02-17
**설계**: 0병력 transfer/march 거부 + 시드 3개 + 순차학습 20게임 (Easy 없이)

```
등급 분포: 13C + 4D + 3F (B등급 0회)
적벽 승률: 65% (이전과 동일)
chibi 승률: 80% (이전 65%에서 개선)
```

| 지표 | 이전 20게임 (시드 2개) | 현재 20게임 (엔진 수정) | 변화 |
|------|----------------------|----------------------|------|
| C등급 | 13 | 13 | 동일 |
| D등급 | 0 | 4 | +4 |
| F등급 | 7 | 3 | **-4** |
| 0병력 transfer 거부 | — | 35회 작동 | 행동 낭비 차단 |

**발견**: 엔진 수정(0병력 거부)이 F→D 전환에 기여. 그러나 C 천장은 그대로.

**추가 분석 — 남군 전투 상세**:
- `gangha→nanjun` (인접): 11건 전투, 전부 패배 (0~188명 vs 15,000명)
- `hagu→nanjun` (비인접): **16회 실패** — 모델이 맵 인접성을 이해하지 못함
- 남군 전투 11건 중 10건: 공격 1~5명 vs 수비 15,000명 (0명 체크 통과)

**근본 원인 3가지**:
1. **보급 역전 미실행**: B시드의 "hagu→gangha 역전" 교훈 무시
2. **hagu→nanjun 비인접 미인지**: 16회 실패에도 계속 시도
3. **소수 병력 진군 통과**: 1~5명이 `> 0` 체크를 통과하여 무의미한 전투 진입

### 2.7 실험 6: MilestoneCoach — **적시 코칭의 성공**

**날짜**: 2026-02-17
**설계**: MidGameReflector를 마일스톤 기반으로 확장. EASY_GUIDE 비활성화.

**코칭 설계 원칙**: "한 번에 한 마디만, 필요한 순간에만"

```
마일스톤 1 (게임 시작): "하구에 병력을 모아라"
마일스톤 2 (chibiVictory 직후): "보급 방향을 역전하라 (hagu→gangha). gangha→nanjun 진군 가능"
마일스톤 3 (강하 병력 충분 시): "남군으로 진군하라"
```

**결과: 5C + 0F + 0D**

| 지표 | 이전 20게임 | MilestoneCoach 5게임 | 효과 |
|------|------------|---------------------|------|
| F등급 | 3 (15%) | **0 (0%)** | 소멸 |
| 0병력 transfer 거부 | 35회 | **0회** | 코칭이 방향 교정 |
| 남군 전투 발생 | 11회 (20게임) | **10회 (5게임)** | 훨씬 적극적 |
| 1차 남군 진군 병력 | 0~188명 | **604~2,208명** | **대폭 개선** |
| hagu→nanjun 실패 | 16회 | 10회 | 여전히 높음 |

**남군 전투 상세**:
```
Game 1 턴12: 604명 vs 14,753명 → 패배
Game 2 턴12: 2,208명 vs 14,243명 → 패배
Game 3 턴12: 1,092명 vs 14,511명 → 패배
Game 4 턴10: 1,849명 vs 14,527명 → 패배
Game 5 턴12: 1,259명 vs 14,609명 → 패배
```

**이것이 이 리포트에서 EASY_GUIDE 역효과와 함께 가장 중요한 발견이다.**

### 2.8 EASY_GUIDE vs MilestoneCoach: 동일 목적, 반대 결과

같은 정보("보급 역전", "남군 진군", "인접성")를 **전달 방식만 바꿔서** 결과가 완전히 뒤집힘:

```
EASY_GUIDE (일괄 설교):    19개 규칙 사전 주입 → 5D (0% 적벽 승률)
MilestoneCoach (적시 코칭): 3개 마일스톤 × 1~2문장 → 5C (100% 적벽 승률, F 소멸)
```

| 비교 항목 | EASY_GUIDE | MilestoneCoach |
|-----------|------------|----------------|
| 정보량 | 19개 지시/금지 | 3개 마일스톤 × 1~2문장 |
| 전달 시점 | 게임 시작 전 일괄 | 해당 상황 도달 시 적시 |
| 금지 리스트 | 5개 | 0개 |
| 모델 반응 | 위축 → march 기피 | 자연스런 행동 전환 |
| 적벽 승률 | 0% | 100% |
| F등급 | — (D라서 해당없음) | 0% |

### 2.9 실험 8: o4-mini + MilestoneCoach (Core 업그레이드 효과)

**날짜**: 2026-02-17
**설계**: o4-mini (reasoning Core) + Seed ICL + MilestoneCoach, normal 난이도

o4-mini는 징병(conscript)을 자체적으로 발견(최대 3회), 병력 4,607명까지 도달했으나:

```
최대 투사 병력: 4,607명 vs 남군 잔존 ~11,500명 (20,000 × 0.5)
결과: 5C, B등급 0회
```

reasoning Core의 독자적 발견(징병)은 있었으나, 현재 난이도에서는 근본적으로 병력이 부족.

### 2.10 실험 9: Easy 커리큘럼 학습 — **🏆 B등급 최초 달성 + A등급 2회**

**날짜**: 2026-02-17
**설계**: 난이도 조정(DifficultyModifier) + Seed ICL + MilestoneCoach, easy 난이도

**난이도 조정 내역**:
- 남군 초기 병력 70% (23,000 → 16,100)
- 적벽 후 collapse 70% (잔존 ~4,830명, 기존 ~11,500명)
- 플레이어 초기 식량 1.5배
- 손권 식량 지원 5,000 (적벽 승리 시 하구 지급)

```
결과: D → F → A → B → A
```

| 게임 | 등급 | B근접도 | 주요 사항 |
|------|------|---------|-----------|
| 1 | D | 10% | develop 12회, 적벽 미도달 |
| 2 | F | 7% | 외교 과다 → 패망 |
| 3 | **A** | **63%** | 적벽 승리 + 남군 점령 + 동맹 유지 |
| 4 | **B** | 48% | 적벽 승리 + 남군 점령 |
| 5 | **A** | **47%** | 적벽 승리 + 남군 점령 + 동맹 유지 |

**핵심 성과**:
- **B등급 최초 달성** (게임 4) — 이전 ~80게임에서 한 번도 없었음
- **A등급 2회** (게임 3, 5) — B를 뛰어넘어 A까지 도달
- 적벽 승률 60% (3/5)
- 학습 곡선: 이동평균 2.0 → 4.7 (**급격한 상승**)
- B근접도: 평균 35%, 최대 63%

**커리큘럼 학습이 작동한 메커니즘**:

```
기존 (normal):
  AI 최대 ~4,600명 vs 남군 잔존 ~11,500명 → 절대 열세 → 항상 패배 → 항상 C

Easy 커리큘럼:
  남군 잔존 ~4,830명 → AI 4,000~5,000명으로 대등한 전투 가능
  → 가끔 승리 → B/A 경험 생성 → Soft Shell에 성공 패턴 축적
  → 게임 3에서 A 달성 → 게임 4,5에서 학습 효과 지속
```

**D→F→A 급등의 의미**:
- 게임 1,2는 학습 데이터 부족으로 실패 (이전 실험과 유사한 패턴)
- 게임 3에서 돌파 — 게임 2의 실패 교훈 + 시드 경험 + 코칭이 결합
- 게임 3의 **실제 A등급 경험**이 시드에 추가되면서 게임 4,5에서 패턴 안정화
- 이것이 바로 **Closed Learning Loop의 탈출**: 처음으로 C 이상의 경험이 Soft Shell에 포함

**이것이 리포트 전체에서 가장 중요한 결과다.**

### 2.11 실험 10: Stage 2 — Easy 시드로 Hard(구normal) 도전

**날짜**: 2026-02-17
**설계**: Easy 커리큘럼에서 생성된 A/B 경험을 시드로 사용, hard 난이도 (구 normal, collapse 50%) 5게임

**핵심 질문**: Easy에서 배운 "보급 역전 + 남군 진군" 패턴이 hard에서도 작동하는가?

```
결과: F → C → C → F → C
```

| 게임 | 등급 | B근접도 | 주요 사항 |
|------|------|---------|-----------|
| 1 | F | 46% | 적벽 승리 후 남군 진군에서 유비 전사 |
| 2 | C | 48% | 적벽 승리, 남군 점령 실패 |
| 3 | C | 53% | 적벽 승리, 남군 점령 실패 |
| 4 | F | 49% | 적벽 승리 후 남군 진군에서 유비 전사 |
| 5 | C | 44% | 적벽 승리, 남군 점령 실패 |

**긍정적 변화 (전략 전이 확인)**:
- 적벽 승률 **100%** (5/5) — 이전 normal 대비 안정적
- B근접도 평균 **48%** (이전 normal 추정 30~35%에서 대폭 상승)
- **모든 게임에서 남군 진군을 시도** — Easy에서 배운 패턴이 전이됨
- B근접도 분산이 작음 (44~53%) — 전략 인식이 균일하게 높음

**돌파 못한 벽**:
- hard 남군 잔존 ~11,500명 vs AI 투사 병력 ~4,000명 — 수적 열세 불변
- F등급 2회: **유비를 진군에 포함**시켜 전사/포로 → 게임오버 (코칭에서 경고하는데도 반복)
- B근접도 48%로 올랐으나 **실제 남군 점령까지는 도달 불가**

**커리큘럼 전이 효과 분석**:

```
이전 hard (MilestoneCoach): 5C, B근접도 추정 30~35%
Stage 2 hard (Easy 시드):    3C+2F, B근접도 평균 48%

→ 전략 인식은 확실히 전이 (+13~18%p B근접도 상승)
→ 그러나 hard의 병력 갭(11,500 vs ~4,000)은 전략만으로 극복 불가
→ 유비 포함 진군 → F 2회 = 새로운 병리 패턴
```

**결론**: Case B에 해당 — "전략 전이 성공, 수치 열세 미해결". 커리큘럼 중간 단계(medium) 필요.

**참고: normal에도 손권 식량 지원 메커니즘이 존재** (`milestones.ts:358-378`):
- 매 턴 1,000 gift — 조건: 동맹 유지 + 턴 6+ + 하구 식량 < 6,000 + 손권 총 식량 ≥ 5,000
- DifficultyModifier의 일회성 sunQuanFoodSupport와는 별개 메커니즘
- 이론적으로 턴 6~20 × 1,000 = 최대 15,000 식량 가능
- 그러나 AI가 하구 식량을 6,000 이상 유지하거나, develop 과다로 식량이 넘치면 발동 안 됨
- 이 메커니즘의 실제 발동 여부를 로그에서 확인해야 함

**여전히 빠진 핵심 요소 — 식량-징병 루프**:
- 인간 A등급 전략: develop → S등급 농업 → 식량-징병 루프 (500→1000명, 1000→2500명)
- AI (Easy A등급): 기존 병력 + transfer로 약화된 남군에 대등. 징병 루프 없이 달성
- **Easy A등급과 인간 A등급은 전혀 다른 전략** — Easy는 환경이 허용, 인간은 전략이 허용
- normal에서 B를 달성하려면 결국 식량-징병 루프를 "발견"해야 함 (또는 손권 지원을 최대한 활용)

### 2.12 난이도 체계 재정의

Stage 2(구 normal) 결과와 구 medium 결과가 동일 수준(3C+2F)이라는 데이터가 난이도 체계 재정의를 촉발:

```
재정의 전:                     재정의 후:
  easy   (collapse 70%)          easy   (collapse 70%)  ← 유지
  medium (collapse 60%)          medium (collapse 65%)  ← 신규 (easy↔normal 사이)
  normal (collapse 50%)    →     normal (collapse 60%)  ← 구 medium
  hard   (collapse 30%)          hard   (collapse 50%)  ← 구 normal
  expert (collapse 20%)          expert (collapse 30%)  ← 구 hard
```

**재정의 근거**: 인간이 A를 달성한 조건이 구 normal(collapse 50%)이므로, 이것은 인간 기준 "hard". 실험 데이터가 이를 뒷받침:
- 구 normal과 구 medium이 같은 결과(3C+2F) → 실질적 난이도 차이 없음
- AI에게 의미 있는 난이도 계단은 잔존 ~5,000~7,000 구간에 존재

### 2.13 실험 11: Medium(재정의) — 커리큘럼 그래디언트 확인

**날짜**: 2026-02-17
**설계**: 재정의된 medium (collapse 65%, 병력 0.8x, 잔존 ~6,440) + 유비 제외 강화 + 징병 코칭

```
결과: A → C → D → C → C
```

| 게임 | 등급 | B근접도 | 주요 사항 |
|------|------|---------|-----------|
| 1 | **A** | 46% | 적벽 승리 + 남군 점령 + 동맹 유지 |
| 2 | C | 53% | 적벽 승리, 남군 미점령 |
| 3 | D | 18% | 외교 과다, 적벽 미도달 |
| 4 | C | 56% | 적벽 승리, 남군 진군 시도 (**5회 전투!**) |
| 5 | C | 50% | 적벽 승리, 남군 진군 시도 |

**핵심 성과**:
- **A등급 1회** — medium에서도 남군 점령 가능 확인
- **F등급 0회** — 유비 제외 코칭 강화 효과 입증 (hard/normal에서 F2 → medium에서 F0)
- B근접도 평균 45% (구 medium 41%에서 상승)
- **게임 4의 5회 전투** — 징병 코칭 효과로 남군 진군을 적극 반복 시도

**커리큘럼 그래디언트 확인**:

| 난이도 | 남군 잔존 | 등급 분포 | B근접도 평균 | B+등급 | F등급 |
|--------|-----------|-----------|--------------|--------|-------|
| easy | ~4,830 | 2A+1B+1D+1F | 35% | 3 | 1 |
| **medium** | **~6,440** | **1A+3C+1D** | **45%** | **1** | **0** |
| normal | ~7,820 | 3C+2F | 41% | 0 | 2 |
| hard | ~11,500 | 3C+2F | 48% | 0 | 2 |

그래디언트가 성립한다:
- **B+등급**: easy(3) > medium(1) > normal(0) = hard(0) — 난이도에 비례하여 감소
- **F등급**: easy(1) > hard(2) = normal(2) > medium(0) — medium에서 F 소멸 (유비 코칭 효과)
- **B근접도**: 난이도와 무관하게 40~48% 범위 — 전략 인식은 전이되었으나 실행 격차 존재

**유비 제외 코칭 + 징병 코칭의 복합 효과**:
```
Stage 2 hard (유비 코칭 약):   F 2회 (유비 전사)
Medium (유비 코칭 강화+징병):   F 0회 + 5회 전투 시도
→ 유비 제외: F 소멸 기여
→ 징병 코칭: 적극적 전투 시도 기여
```

### 2.14 실험 12: Stage 3 — Normal 도전

**날짜**: 2026-02-17
**설계**: Medium A 시드 + 유비 제외 코칭 + 징병 코칭, normal (collapse 60%, 잔존 ~7,820)

```
결과: D → C → C → C → C
```

| 게임 | 등급 | 적벽 | B근접도 | 주요 사항 |
|------|------|------|---------|-----------|
| 1 | D | X | 14% | 적벽 패배 |
| 2 | C | O | 54% | 적벽 승리, 남군 미점령 |
| 3 | C | O | 48% | 〃 |
| 4 | C | O | 60% | 남군 진군 시도했으나 병력 부족 |
| 5 | C | O | 53% | 〃 |

**핵심 성과**:
- **F등급 완전 제거** (이전 hard에서 F3 → normal에서 F0)
- B근접도 평균 46% (medium 45%와 동등)
- 게임 4에서 B근접도 60% — 진군 시도는 했으나 병력 부족

**커리큘럼 전체 비교 (최종)**:

| 난이도 | 남군 잔존 | 등급 분포 | B+등급 | F등급 | B근접도 평균 |
|--------|-----------|-----------|--------|-------|--------------|
| easy | ~4,830 | 2A+1B+1D+1F | 3 | 1* | 35% |
| medium | ~6,440 | 1A+3C+1D | 1 | 0 | 45% |
| **normal** | **~7,820** | **4C+1D** | **0** | **0** | **46%** |
| hard | ~11,500 | 2C+3F | 0 | 3 | 47% |

*easy F1은 유비 코칭 강화 이전

**B근접도 포화 현상**: 45~47%로 난이도 무관하게 수렴. 이는 **전략은 동일하게 전이되었으나, AI의 병력 확보 능력(~4,000~5,000명)이 상한**이라는 것을 의미. 난이도를 내리면(남군 약화) B/A가 가능하지만, 난이도를 올리면 동일한 전략이 수치 열세로 실패.

**발견 7: 전략 전이 ≠ 성과 전이**

```
전략 전이: ✅ — B근접도 45~47%, 모든 난이도에서 동일한 전략 패턴
성과 전이: ❌ — B+등급은 남군 잔존 ~6,500 이하에서만 달성

→ 전략을 "알고" 있어도, 실행에 필요한 자원(병력)이 없으면 성과로 이어지지 않음
→ 이것은 "전략의 한계"가 아니라 "자원 확보 능력의 한계"
→ 다음 돌파구: 남군을 더 약화시키는 것이 아니라, AI의 병력 확보 능력을 높이는 것
```

**C→B 임계점 분석**:
```
easy   잔존 ~4,830 vs AI ~4,500 = 1:1.1 → B/A 가능
medium 잔존 ~6,440 vs AI ~4,500 = 1:1.4 → A 가끔 가능
normal 잔존 ~7,820 vs AI ~4,500 = 1:1.7 → C 한계
hard   잔존 ~11,500 vs AI ~4,500 = 1:2.6 → C 한계

→ 임계 비율: ~1:1.4 이하에서 B 달성 가능
→ AI 병력을 ~6,000으로 올리면 normal에서도 B 가능 (1:1.3)
→ 징병 루프가 핵심: conscript 3~5회 → +1,500~2,500명 → 총 6,000~7,000명
```

---

## 3. 핵심 발견: Soft Shell 과적합과 적시 코칭 (Overfitting & Timely Coaching)

### 3.1 현상

```
seed 3개 (normal):          4C + 1F ← 적벽 승리 잘 함
seed 3개 + EASY_GUIDE (easy): 5D     ← march 자체를 안 함
```

EASY_STRATEGY_GUIDE를 추가하면 모델이 **march를 하지 않게 된다**.

### 3.2 원인 분석

EASY_STRATEGY_GUIDE의 구조:

```
1단계: ~하라 (3개 지시)
2단계: ~하라 (3개 지시)
3단계: ~하라 (3개 지시)
4단계: ~하라 (5개 지시)
절대 하지 말 것: 5개 금지
→ 총 19개 지시/금지 사항
```

**경량 모델(Gemini Flash)에서의 반응**:
- 지시가 많아지면 "안전한 선택"(내정 반복)으로 회귀
- 특히 "절대 하지 말 것" 리스트가 **금지 회피 행동**을 유발
- "병력 없이 진군하면 반드시 패배" → march 자체를 기피
- 결과: Level 1("준비만 함")으로 퇴행

### 3.3 Four-Shell Model 해석

```
                Seed ICL (시범)              EASY_GUIDE (설교)
Shell 유형      Soft Shell                   Hard Shell 강화
정보 형태       과거 전역의 경험 서사         명시적 규칙/금지 리스트
모델 반응       패턴 모방 (암묵적 학습)      규칙 준수 시도 (과부하)
결과            D→C 도약                     C→D 퇴행
```

> **발견: 경량 모델에게 "시범"은 효과적이지만 "설교"는 역효과다.**

이는 인간 학습에서도 관찰되는 패턴:
- 초보자에게 규칙을 나열하면 위축됨 (analysis paralysis)
- 전문가의 플레이를 보여주면 패턴을 자연스럽게 습득함
- 금지 사항이 많을수록 행동 범위가 축소됨

### 3.4 발견: 적시 코칭이 일괄 설교를 압도

EASY_GUIDE의 역효과를 MilestoneCoach가 해결:

```
설교 (EASY_GUIDE):        19개 규칙 일괄 → 5D (역효과)
시범 (Seed ICL):           경험 서사 → 4C+1F (효과)
적시 코칭 (MilestoneCoach): 상황별 1~2문장 → 5C+0F (시범+코칭 시너지)
```

**정보의 양이 아니라 전달 타이밍이 결정적이다.**
동일한 내용("보급 역전", "gangha→nanjun 경로")이라도:
- 게임 시작 전 일괄 제공 → 과부하, 위축
- 해당 상황 도달 시 1문장 제공 → 즉시 행동 전환

이는 인간 코칭에서의 "just-in-time instruction" 원칙과 일치한다.

### 3.5 일반화 가능한 원칙

| 원칙 | 설명 | 근거 |
|------|------|------|
| **시범 > 설교** | 경험 서사가 명시적 규칙보다 효과적 | Seed ICL +100% vs EASY_GUIDE -100% |
| **적시 코칭 > 일괄 설교** | 같은 정보도 타이밍이 결과를 뒤집음 | EASY_GUIDE 5D vs MilestoneCoach 5C |
| **Hard Shell 두께 ∝ 1/Core** | Core가 약할수록 Hard Shell을 얇게 해야 함 | Flash에서 19개 지시 → 과부하 |
| **금지 리스트의 역설** | "하지 마라"가 많을수록 행동 위축 | 5개 금지 → march 기피 |
| **Soft Shell 포화** | 동일 등급 경험만 축적하면 학습 정체 | 20게임 C-only loop |

### 3.6 교수법 스펙트럼 (Teaching Method Spectrum)

```
효과 없음                     효과 있음                    최대 효과
    ←─────────────────────────────────────────────────────────────→

일괄 설교        시범(ICL)        적시 코칭         시범+코칭          커리큘럼
(EASY_GUIDE)    (Seed ICL)      (MilestoneCoach)  (Coach+Seed)      (+Easy난이도)
  5D               4C+1F            5C               5C+0F            2A+1B 🏆
  역효과           D→C              F 소멸            최고 성과         천장 돌파
```

이 스펙트럼은 Four-Shell Model에 **교수법 차원**을 추가한다:
- Shell의 종류(Hard/Soft)뿐 아니라 **전달 방식**(일괄/적시)이 Phenotype을 결정
- 최적 조합: Soft Shell(시범) + 적시 Hard Shell(마일스톤 코칭)
- **최종 돌파**: 위 조합 + Hardware(환경 난이도) 조정 = 커리큘럼 학습

---

## 4. 실패 스펙트럼 업데이트

Phase 2에서 정의한 4단계 실패 스펙트럼에 ICL 효과를 추가:

```
Level 0: 준비도 안 함     — Qwen/Exaone/Llama (로컬 SLM)
  ICL 효과: 미측정

Level 1: 준비만 함        — Gemini Flash (ICL 없음)
  transfer 33, march 0
  ICL 효과: → Level 2로 도약 (Seed ICL)
  ICL 역효과: → Level 1로 퇴행 (EASY_GUIDE)

Level 1.5: 소극적 시도    — Haiku 4.5 / Sonnet 4.5
  transfer 13~34, march 2 (비전략적)
  ICL 효과: 미측정

Level 2: 전략적 실행      — o4-mini / Gemini Flash+ICL
  transfer → 하구 집중, 적벽 방향 march
  3단계 전략: 내정 → 집결 → 진군

Level 2.5: 적벽 후 실행 불가 — normal 난이도 ICL 천장
  적벽 승리 후 보급 역전 + 남군 점령을 못 함
  B등급 도달에 필요한 "4단계 전략" 미학습

Level 3: 환경 지원 전략    — B/A등급 (Easy 커리큘럼으로 달성! 🏆)
  적벽 승리 + 보급 역전 + 남군 점령
  단, 약화된 남군(~4,830명) 상대로 달성 — 기존 병력으로 충분

Level 4: 자립 전략         — B등급 이상, normal (미달성)
  식량-징병 루프로 병력 증강 후 남군 점령
  인간 A등급 전략 수준
```

---

## 5. C→B 천장: "Closed Learning Loop" 문제

### 5.1 문제 구조

```
C등급 게임 반복
  → 경험 추출: "적벽 승리했으나 남군 미점령"
  → 교훈 생성: "보급으로 하구에 집중한 것이 적벽 승리의 핵심"
  → 다음 게임: 동일한 "하구 집중 → 적벽 승리" 전략 반복
  → 결과: 또 C등급
  → 새 교훈: 기존과 동일 (중복 제거됨)
  → 학습 포화
```

**근본 문제**: B등급 경험이 학습 데이터에 없으므로, "적벽 후 전략"을 학습할 수 없다.

### 5.2 합성 시드의 한계

B등급 합성 시드를 추가했지만 효과 없음. 가능한 원인:

1. **경험 서사의 신뢰도**: 합성 경험이 실제 게임 경험만큼 설득력이 없을 수 있음
2. **단일 시드 부족**: 1개의 B등급 시드 vs 2개의 C등급 시드 + N개의 실제 C등급 경험 — B패턴이 C패턴에 묻힘
3. **2단계 로직 한계**: "보급 역전(hagu→gangha) → 남군 진군"은 2-step sequential planning으로, 경량 모델의 reasoning 한계일 가능성

### 5.3 미해결 질문

- o4-mini에 ICL을 적용하면 B등급 가능한가? (reasoning + ICL 시너지)
- B등급 합성 시드를 3개 이상으로 늘리면 돌파 가능한가?
- 중간 반성(MidGameReflector)의 적벽 후 트리거가 도움이 되는가?
- 근본적으로 "2단계 sequential planning"이 경량 모델의 Core 한계인가?

---

## 6. Four-Shell Model에 대한 시사점

### 6.1 검증된 것

| 가설 | 결과 | 의미 |
|------|------|------|
| Soft Shell이 Phenotype을 변화시키는가? | ✅ **강력하게 검증** | 0% → 100% 적벽 승률 (Seed ICL) |
| 다른 AI의 경험을 이식 가능한가? | ✅ **검증** | o4-mini 경험 → Gemini Flash에 전이 성공 |
| Core 한계를 Soft Shell이 보완하는가? | ⚠️ **부분 검증** | D→C 보완 가능, C→B는 환경 조정도 필요 |
| Hard Shell 강화가 Soft Shell을 대체하는가? | ❌ **반증** | EASY_GUIDE가 오히려 역효과 |
| 적시 코칭이 일괄 설교보다 효과적인가? | ✅ **강력하게 검증** | MilestoneCoach 5C vs EASY_GUIDE 5D |
| Hardware 조정이 Shell 효과 상한을 바꾸는가? | ✅ **강력하게 검증** | Easy 커리큘럼: 0B → 1B+2A 🏆 |
| 커리큘럼 학습이 Closed Loop를 탈출시키는가? | ✅ **검증** | 첫 A경험 → 이후 B/A 안정화 |

### 6.2 새로 발견된 것

#### 발견 1: "시범 > 설교" 원칙

경량 모델에게 경험 서사(Soft Shell)는 효과적이지만, 명시적 규칙(Hard Shell 강화)은 역효과.

```
Soft Shell (시범):  "지난 전역에서 턴 10에 하구→적벽으로 진군하여 승리했다"
  → 모델: "턴 10쯤 진군해봐야겠다" (패턴 모방)

Hard Shell (설교):  "턴 12 이후 반드시 진군하라. 보급 없이 진군하면 패배한다. 0명 진군 금지."
  → 모델: "진군하면 실패할 수 있으니 더 준비하자" (회피 행동)
```

#### 발견 2: Shell 간 간섭 효과

Hard Shell을 두껍게 하면 Soft Shell의 효과가 상쇄된다.

```
Soft Shell만:             D → C (효과 +2등급)
Soft + 일괄 Hard Shell:    D → D (효과 0, 간섭)
Soft + 적시 Hard Shell:    C → C, F소멸 (시너지)
```

이는 프롬프트 엔지니어링에서 "지시가 많을수록 좋다"는 통념에 반하는 결과.
단, **적시 전달(MilestoneCoach)**은 간섭 없이 시너지를 만든다 — 핵심은 양이 아니라 타이밍.

#### 발견 3: Closed Learning Loop

동일 등급의 경험만 축적하면 학습이 포화한다. 돌파(breakthrough)를 위해서는 **더 높은 등급의 경험**이 필요하지만, 현재 등급에서는 그 경험을 생성할 수 없다.

이는 인간 학습의 "zone of proximal development" (비고츠키)와 유사:
- 현재 능력(C등급)으로 도달 가능한 경험만 축적됨
- 다음 단계(B등급)는 외부 도움(교사/시드) 없이는 도달 불가
- 합성 시드(인공적 B등급 경험)가 "교사 역할"을 하지 못한 것은 서사적 설득력의 문제일 수 있음

#### 발견 4: 적시 코칭 (MilestoneCoach)

상황 도달 시 1~2문장의 조언을 주입하는 방식이 일괄 가이드보다 압도적으로 효과적.

```
일괄 설교 (19개 규칙):     → 과부하 → march 기피 → D
적시 코칭 (3 마일스톤):     → 상황 인식 → 행동 전환 → C, F소멸
```

이는 "just-in-time instruction" 원칙의 AI 버전:
- 정보 과잉은 약한 Core에서 decision paralysis를 유발
- 상황에 맞는 최소 정보는 즉각적 행동 변화를 만듦
- 금지("하지 마라") 대신 지시("이걸 하라")가 효과적

### 6.3 Core-Soft Shell 상호작용 맵 (업데이트)

```
              ICL 전         Seed ICL       EASY_GUIDE     MilestoneCoach  Easy커리큘럼   Hard전환     Medium
Flash         0% (5D)        100% (4C+1F)   0% (5D) ↓     100% (5C)       60% (2A+1B)   100% (3C+2F) 60% (1A+3C+1D)
Haiku         0% (5D)        미측정          미측정          미측정           미측정          미측정        미측정
Sonnet        0% (4D+1F)     미측정          미측정          미측정           미측정          미측정        미측정
o4-mini       33% (2C+4D)    100% (5C)      미측정          100% (5C)       미측정          미측정        미측정
```

**Easy 커리큘럼**: Flash가 **처음으로 B/A 등급 달성**. Hardware(게임 환경) 조정이 Phenotype 천장을 돌파.
**Hard(구normal) 전환**: 전략 전이 확인 (B근접도 48%), 그러나 B등급 미달.
**Medium**: A 1회 달성, F 소멸. 커리큘럼 그래디언트 성립 확인.

### 6.4 발견 5: Sparse Reward 문제와 커리큘럼 학습의 필요성

C→B 천장이 깨지지 않는 근본 원인은 **학습 환경의 구조적 문제**:

#### Sparse Reward (희소 보상)

현재 보상 체계는 S/A/B/C/D/F 6단계 이산 등급뿐이다.

```
C등급 게임 A: 남군 604명 vs 14,753명 → "C"
C등급 게임 B: 남군 4,607명 vs 20,023명 → "C"
→ 둘 다 동일한 "C" — 게임 B가 6배 나은데 구별 불가
```

AI는 "또 C"만 보기 때문에 **어떤 방향으로 개선해야 하는지 gradient가 없다**.
강화학습에서 sparse reward(승/패만 존재)가 학습을 극도로 어렵게 만드는 것과 동일한 구조.

#### Closed Learning Loop의 진짜 원인

```
이전 진단: "B경험이 없어서 B를 학습 못 함" (데이터 부족)
추가 진단: "C 안에서의 진전이 보이지 않아서 방향을 못 잡음" (보상 신호 부족)
```

두 문제가 결합되어 학습 정체를 만든다:
1. 등급이 같으면 경험이 구별되지 않음 → **보상 형성(reward shaping)** 필요
2. B가 현재 난이도에서 거의 불가능 → **커리큘럼 학습(curriculum learning)** 필요

#### 커리큘럼 학습 (비고츠키 ZPD의 게임 환경 구현)

```
현재: 매번 동일한 "불가능에 가까운" 환경
  → C만 반복 → Closed Learning Loop

필요: 달성 가능한 환경에서 성공 경험 생성 → 시드화 → 점진적 난이도 상승
  난이도 1: 남군 약화 (-70%) → B 달성 가능 → B 경험 축적
  난이도 2: 보통 (-50%)     → B 시드로 학습
  난이도 3: 원래 난이도      → 학습된 전략으로 도전
```

이는 아이를 키울 때와 동일한 원리:
- 불가능한 과제를 반복시키면 → 좌절만 반복, 학습 없음
- 약간 어려운 과제를 주면 → 가끔 성공 → 성공 경험이 다음 도전의 기반
- 바로 이것이 비고츠키의 **근접 발달 영역(ZPD)**: 혼자서는 못 하지만 도움 있으면 가능한 영역

Four-Shell Model 관점에서, 이는 **Hardware(게임 환경) 조정**에 해당:

```
Hardware: 게임 엔진 + 시나리오 밸런스 ← 난이도 조정 (커리큘럼)
Core: LLM 가중치 (고정)
Hard Shell: 프롬프트 + MilestoneCoach (고정)
Soft Shell: ICL 경험 ← 보상 형성으로 품질 향상
Phenotype: 등급 (관측)
```

#### "B등급 근접도" — Shaped Reward 설계

등급 외에 연속적인 진전 지표를 경험에 포함:

```
B등급 근접도 (0~100%):
  chibiVictory:           +30%
  남군 진군 시도:          +10%
  남군 전투 병력 비율:     +0~30% (자군/적군 비율에 비례)
  징병 > 0회:             +10%
  develop > 2회:          +10%
  보급 역전 실행:          +10%
```

이를 통해 ExperienceStore.selectBalanced()가 "B근접도 최고" 경험을 role model로 선택 → 점진적 수렴 가능.

#### 인간 전략 참조 (게임 디자이너 A등급 달성 사례)

게임 디자이너가 직접 플레이하여 A등급까지 달성한 전략:

```
1. 초반: develop → 양 도시 농업 S등급 (식량 생산 극대화)
2. 중반: 군량미 보면서 지속적 conscript (식량-징병 루프)
   - 식량 500 → 징병 1,000명
   - 식량 1,000 → 징병 2,500명
3. 적벽: 동남풍 + 화공 3연속 → 조조 병력 반감
4. 적벽 후: 식량-징병 루프 극대화
   - 하구/강하 식량 0까지 징병 → 전부 강하로 집중
   - 하구 농업생산으로 식량 회복 → 다시 징병
   - 남군 수비와 비슷한 규모 될 때까지 반복
5. 전 병력 강하 집중 → main scale march → 남군 점령
```

이 전략의 핵심은 **식량-징병 루프**: `while (병력 < 남군) { if (식량 >= 500) conscript(); transfer(→강하); }`

이는 현재 AI가 전혀 수행하지 못하는 **반복적 자원 관리 루프**이며, ICL/코칭으로 가르치기 가장 어려운 패턴. 커리큘럼 학습 + 보상 형성이 이 루프의 "발견"을 가능하게 할 수 있는지가 다음 실험의 핵심 질문.

### 6.5 발견 6: 커리큘럼 학습(Hardware 조정)의 결정적 효과 🏆

Easy 커리큘럼 5게임 결과가 이 리포트의 **가장 중요한 발견**이다:

```
이전 ~80게임 (normal): B등급 0회, 최고 C등급
Easy 커리큘럼 5게임:    B등급 1회, A등급 2회 ← 최초 돌파
```

#### Four-Shell 관점에서의 의미

```
기존 접근: Core/Hard Shell/Soft Shell 조정 → C 천장 돌파 실패
이번 접근: Hardware(게임 환경) 조정 → B/A 달성

→ Four-Shell Model에서 Hardware(환경)는 다른 모든 Shell의 효과 상한을 결정한다.
→ 아무리 좋은 Soft Shell + Hard Shell이어도, Hardware가 불가능한 환경이면 학습 불가.
```

이는 인간 교육의 **환경 설계** 원칙과 정확히 일치:
- 수영 못하는 아이를 깊은 물에 넣으면 → 공포만 학습 (normal에서 C 반복)
- 얕은 물에서 시작하면 → 수영을 학습 (easy에서 A 달성)
- 얕은 물 경험이 깊은 물 도전의 기반이 됨 (Stage 2 커리큘럼 전환)

#### 학습 곡선의 의미

```
게임 1: D (2.0) — 이전 패턴 반복
게임 2: F (1.0) — 탐색 실패
게임 3: A (5.0) — 돌파! 적벽 후 남군 점령 성공 ← 최초의 B+ 경험
게임 4: B (4.0) — 게임 3의 A경험을 Soft Shell에 반영 → 패턴 재현
게임 5: A (5.0) — 패턴 안정화
이동평균: 2.0 → 1.5 → 2.7 → 3.3 → 4.7 (급격 상승)
```

게임 3의 돌파가 결정적이었다. 이 한 번의 성공이:
1. **Closed Learning Loop 탈출** — 처음으로 C 이상의 경험이 Soft Shell에 포함
2. **성공 패턴 이식** — 게임 4,5에서 동일 패턴 재현 (시드 효과)
3. **학습 곡선 전환** — 하강→상승 전환점

이것이 바로 커리큘럼 학습의 핵심: **"한 번의 성공이 모든 것을 바꾼다"**.

#### 일반화 원칙

| 원칙 | 설명 | 근거 |
|------|------|------|
| **Hardware ≥ Shell** | 환경 난이도가 Shell 효과의 상한을 결정 | normal 80게임 0B vs easy 5게임 3B+ |
| **커리큘럼 > 반복** | 난이도 조정이 동일 난이도 반복보다 효과적 | easy 5게임 > normal 20게임 순차학습 |
| **한 번의 성공 = 학습 촉매** | 첫 성공 경험이 이후 학습을 가속 | 게임 3 A → 게임 4,5 B/A 안정화 |
| **Closed Loop 탈출 = 환경 변경** | 같은 환경에서는 같은 결과만 반복 | 환경(Hardware) 조정이 유일한 탈출구 |

---

## 7. 향후 실험 계획

### 7.1 즉시 실행: 병력 확보 능력 향상

| 우선순위 | 실험 | 목적 | 비용 |
|---------|------|------|------|
| **1** | **징병 루프 코칭 강화** | 1회 코칭 → 반복 코칭으로 전환, conscript 3~5회 유도 | — |
| 2 | Normal + 강화 징병코칭 5게임 | AI 병력 ~6,000 도달 가능한지 확인 | ~$0.20 |
| 3 | Medium + 강화 징병코칭 5게임 (대조군) | 징병 코칭 강화의 순수 효과 측정 | ~$0.20 |

### 7.2 커리큘럼 학습 로드맵

```
Stage 1: Easy 난이도 (✅ 완료)
  → B등급 최초 달성, A등급 2회 (2A+1B+1D+1F)
  → 핵심: 환경 조정으로 "성공 가능한" Zone of Proximal Development 생성

Stage 2: Easy → Hard(구normal) 직행 (✅ 완료 — 부분 성공)
  → 전략 전이 확인: B근접도 30~35% → 48% 상승
  → 그러나 B등급 미달: 남군 11,500명 vs 4,000명 = 수적 열세 불변
  → 유비 포함 진군 → F 2회 = 새 병리 패턴
  → 교훈: easy→hard 점프가 너무 큼 (잔존 4,830 → 11,500 = 2.4배)

Stage 2.5: Medium 난이도 (✅ 완료 — 성공)
  → A 1회, F 0회 (1A+3C+1D)
  → 유비 제외 코칭 강화 → F 소멸
  → 징병 코칭 → 적극적 전투 시도 (5회 전투)
  → 커리큘럼 그래디언트 성립 확인: easy(3B+) > medium(1B+) > normal(0) = hard(0)

Stage 3: Medium → Normal 전환 (✅ 완료 — 전략 전이 성공, B 미달)
  → 4C+1D, F 0회, B근접도 46%
  → 전략 전이 확인, 그러나 병력 갭(~7,820 vs ~4,500)으로 B 불가
  → B근접도 45~47%로 포화 — "전략의 한계"가 아닌 "자원 확보의 한계"

★ 방향 전환: 난이도 추가 하강 대신 AI 병력 확보 능력 향상
  → 핵심: 징병(conscript) 루프 발견 유도
  → 현재: 1회 코칭 → 1~2회 징병 → +500~1,000명
  → 목표: 반복 코칭 → 3~5회 징병 → +1,500~2,500명 → 총 ~6,000명
  → 임계 비율 1:1.4 이하 달성 시 normal에서도 B 가능

Stage 4: Normal에서 B 달성 (다음 목표)
  → 징병 루프 코칭 강화 후 재도전
  → AI 병력 ~6,000 달성 시 normal(~7,820)에서 B 가능

Stage 5: Normal → Hard 전환
  → normal B 시드를 들고 hard(잔존 ~11,500) 도전
  → 병력 ~8,000+ 필요 → 식량-징병 루프 완전 학습이 전제

Stage 6: Hard에서 B 달성 (최종 목표)
  → 인간 A등급 전략 수준에 근접
```

### 7.3 난이도별 결과 종합

| 난이도 | 남군 잔존 | 등급 분포 | B근접도 평균 | B+등급 | F등급 | 상태 |
|--------|-----------|-----------|--------------|--------|-------|------|
| easy | ~4,830 | 2A+1B+1D+1F | 35% | 3 | 1* | ✅ 완료 |
| medium | ~6,440 | 1A+3C+1D | 45% | 1 | 0 | ✅ 완료 |
| normal | ~7,820 | 4C+1D | 46% | 0 | 0 | ✅ 완료 (B 미달) |
| hard | ~11,500 | 2C+3F | 47% | 0 | 3 | 대기 |

*easy F1은 유비 코칭 강화 이전

### 7.4 누적 교훈

```
검증된 것:
  1. 커리큘럼 학습이 작동한다 (easy→medium→normal 그래디언트)
  2. 전략 전이는 되지만 성과 전이는 환경에 의존 (B근접도 ↑ ≠ B등급 ↑)
  3. 유비 제외 코칭 강화가 F 소멸에 결정적 (hard F3 → easy~normal F0)
  4. 징병 코칭이 적극적 전투 시도를 유발 (5회 전투)
  5. 합성 시드(효과 0) < 실제 A 시드(효과 확인) — 서사적 설득력 차이
  6. B근접도 45~47% 포화 — 전략이 아닌 병력 확보가 병목
  7. B 달성 임계 비율: AI:남군 ≈ 1:1.4 이하

현재 벽:
  - AI 투사 병력 상한 ~4,000~5,000명 — 초기 병력 + transfer만으로는 한계
  - 징병(conscript) 루프 미발견 — 1회 코칭으로는 1~2회 징병에 그침
  - 난이도를 더 내리는 것은 해결책이 아님 — AI의 능력을 올려야 함

다음 돌파구:
  - 징병 코칭 강화: 1회 발동 → 반복 발동으로 전환
  - 목표: conscript 3~5회 → +1,500~2,500명 → 총 ~6,000명
  - normal 잔존 ~7,820 vs AI ~6,000 = 1:1.3 → B 가능
```

---

## 8. 실험 데이터 참조

### 배치 결과 파일

| 파일 | 실험 | 핵심 데이터 |
|------|------|------------|
| `batch-2026-02-16T10-46-09-868Z.json` | Gemini Flash 기준선 5게임 | 5D, transfer 33, march 0 |
| `batch-2026-02-16T11-15-26-945Z.json` | Claude Haiku 기준선 5게임 | 5D, march 2 (비전략적) |
| `batch-2026-02-16T11-49-15-636Z.json` | Claude Sonnet 기준선 5게임 | 4D+1F, 62.6% 불완전 턴 |
| `batch-2026-02-16T13-31-10-789Z.json` | Seed ICL 5게임 | **4C+1F, 100% chibiVictory** |
| `batch-2026-02-16T13-31-48-074Z.json` | 대조군 5게임 | 3D+2F |
| `batch-2026-02-16T13-59-20-452Z.json` | 순차학습 20게임 | 13C+7F, B등급 0 |
| `batch-2026-02-16T15-02-09-143Z.json` | 엔진 수정 후 5게임 | 4C+1F |
| (2026-02-17 배치) | Easy 모드 5게임 | **5D — 역효과** |
| (2026-02-17 배치) | 엔진 수정 순차학습 20게임 | 13C+4D+3F |
| (2026-02-17 배치) | MilestoneCoach 5게임 | 5C+0F — F 소멸 |
| (2026-02-17 배치) | o4-mini + MilestoneCoach 5게임 | 5C, 최대 4,607명 |
| (2026-02-17 배치) | **🏆 Easy 커리큘럼 5게임** | **2A+1B+1D+1F — 최초 B/A 달성** |
| (2026-02-17 배치) | Stage 2: Hard(구normal) 전환 5게임 | 3C+2F, B근접도 평균 48% |
| (2026-02-17 배치) | **Stage 2.5: Medium(재정의) 5게임** | **1A+3C+1D, F0, B근접도 45%** |
| (2026-02-17 배치) | Stage 3: Normal 5게임 | 4C+1D, F0, B근접도 46% |

### 경험 저장소 파일

| 파일 | 내용 |
|------|------|
| `experience-gemini-3-flash-preview-1771248708071.json` | 시드 ICL 5게임 경험 |
| `experience-gemini-3-flash-preview-1771250360450.json` | 순차학습 20게임 누적 경험 |
| `experience-gemini-3-flash-preview-1771253701213.json` | 엔진 수정 후 경험 |
| `experience-gemini-3-flash-preview-1771254129140.json` | Easy 모드 경험 |

---

## 부록 A: 용어 정의

| 용어 | 정의 |
|------|------|
| **Core** | LLM 모델 가중치 — 변경 불가 |
| **Hard Shell** | 프롬프트, 페르소나, 규칙 — 개발자가 설정 |
| **Soft Shell** | 축적된 게임 경험(GameExperience[]) — 실행 중 학습 |
| **Phenotype** | 관측 가능한 행동 결과 — 등급, 승률, 행동 패턴 |
| **ICL (In-Context Learning)** | 프롬프트에 과거 경험을 포함하여 행동을 유도하는 기법 |
| **Seed ICL** | 다른 AI의 경험을 시드로 사전 주입하는 방식 |
| **Closed Learning Loop** | 동일 등급 경험만 축적 → 동일 등급만 반복하는 정체 현상 |
| **시범 > 설교** | 경험 서사가 명시적 규칙보다 경량 모델에 효과적이라는 원칙 |
| **적시 코칭** | 마일스톤 도달 시 1~2문장 조언을 주입하는 방식 (MilestoneCoach) |
| **MilestoneCoach** | MidGameReflector의 확장. 게임 내 마일스톤 기반 적시 조언 시스템 |
| **교수법 스펙트럼** | 일괄 설교 < 시범(ICL) < 적시 코칭 < 시범+적시 코칭 < 커리큘럼 순서의 효과 스펙트럼 |
| **커리큘럼 학습** | 난이도를 점진적으로 상승시켜 성공 경험을 축적하는 학습법. Hardware 조정에 해당 |
| **DifficultyModifier** | 게임 환경(남군 병력, 식량 등)을 난이도에 따라 변형하는 시스템 |
| **B근접도 점수** | 0~100% 연속 척도로 B등급 근접 정도를 측정. Sparse Reward 문제 해결용 |
| **Hardware ≥ Shell** | 환경 난이도가 모든 Shell 효과의 상한을 결정한다는 원칙 |

## 부록 B: 등급 기준 (Victory Judge)

| 등급 | 조건 |
|------|------|
| **S** | 적벽 + 남군 + 강릉 + 동맹 유지 + 장수 손실 0 |
| **A** | 적벽 + 남군 + 동맹 유지 |
| **B** | 적벽 + 도시 점령 (남군/강릉 중 하나 이상) |
| **C** | 적벽 승리만 달성 |
| **D** | 생존 (적벽 패배/미진행) |
| **F** | 유비 포로/사망 또는 도시 전부 상실 |
